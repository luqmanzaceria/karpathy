{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E01: Trigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "\n",
    "1. With Counting\n",
    "2. With Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram With Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N = torch.zeros((27, 27 ,27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loss(input):\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for w in input:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "            prob = P[ix1, ix2, ix3]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "            #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "    print(f'{log_likelihood=}')\n",
    "    nll = -log_likelihood\n",
    "    print(f'{nll=}')\n",
    "    print(f\"{nll/n=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rexza.\n",
      "ioulius.\n",
      "ila.\n",
      "ityharlonimittain.\n",
      "luwak.\n",
      "=========\n",
      "LOSS\n",
      "log_likelihood=tensor(-105.8303)\n",
      "nll=tensor(105.8303)\n",
      "nll/n=tensor(2.5198)\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "names = []\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0  # Start with the token '.'\n",
    "    \n",
    "    # First character after the start token\n",
    "    p = P[ix1, :].sum(0)  # Aggregate across all potential second characters\n",
    "    ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix2])\n",
    "\n",
    "    while True:\n",
    "        p = P[ix1, ix2]  # Get the probability distribution for the next character\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix3])\n",
    "        if ix3 == 0:  # End token\n",
    "            break\n",
    "        ix1, ix2 = ix2, ix3  # Move to the next character in the trigram\n",
    "    names.append(''.join(out))\n",
    "    print(''.join(out))\n",
    "\n",
    "print(\"=========\")\n",
    "print(\"LOSS\")\n",
    "count_loss(names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  392226\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1, ix2))\n",
    "    ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.195971488952637\n",
      "3.3653788566589355\n",
      "3.049534320831299\n",
      "2.8784797191619873\n",
      "2.7739577293395996\n",
      "2.7012393474578857\n",
      "2.6454999446868896\n",
      "2.601283550262451\n",
      "2.5652339458465576\n",
      "2.535409688949585\n",
      "2.510397434234619\n",
      "2.4892261028289795\n",
      "2.4711146354675293\n",
      "2.455474853515625\n",
      "2.4418272972106934\n",
      "2.4298088550567627\n",
      "2.4191300868988037\n",
      "2.4095730781555176\n",
      "2.400963306427002\n",
      "2.3931643962860107\n",
      "2.386066198348999\n",
      "2.3795783519744873\n",
      "2.373626470565796\n",
      "2.368147611618042\n",
      "2.3630881309509277\n",
      "2.358403205871582\n",
      "2.354053020477295\n",
      "2.3500044345855713\n",
      "2.3462281227111816\n",
      "2.342698097229004\n",
      "2.3393924236297607\n",
      "2.3362908363342285\n",
      "2.3333756923675537\n",
      "2.330632209777832\n",
      "2.3280460834503174\n",
      "2.3256044387817383\n",
      "2.3232970237731934\n",
      "2.321112632751465\n",
      "2.3190431594848633\n",
      "2.317079782485962\n",
      "2.3152151107788086\n",
      "2.313441753387451\n",
      "2.3117542266845703\n",
      "2.3101463317871094\n",
      "2.30861234664917\n",
      "2.307147979736328\n",
      "2.3057494163513184\n",
      "2.3044114112854004\n",
      "2.303130626678467\n",
      "2.301903486251831\n",
      "2.300727128982544\n",
      "2.2995975017547607\n",
      "2.298513412475586\n",
      "2.297471523284912\n",
      "2.296468734741211\n",
      "2.295503854751587\n",
      "2.294574737548828\n",
      "2.2936789989471436\n",
      "2.2928154468536377\n",
      "2.2919814586639404\n",
      "2.2911765575408936\n",
      "2.2903988361358643\n",
      "2.289646625518799\n",
      "2.288919448852539\n",
      "2.288215160369873\n",
      "2.2875335216522217\n",
      "2.2868733406066895\n",
      "2.2862331867218018\n",
      "2.2856123447418213\n",
      "2.2850098609924316\n",
      "2.2844254970550537\n",
      "2.283857583999634\n",
      "2.283306121826172\n",
      "2.2827701568603516\n",
      "2.2822492122650146\n",
      "2.281742572784424\n",
      "2.2812490463256836\n",
      "2.2807693481445312\n",
      "2.280301570892334\n",
      "2.279846668243408\n",
      "2.2794032096862793\n",
      "2.278970718383789\n",
      "2.2785494327545166\n",
      "2.2781386375427246\n",
      "2.277737617492676\n",
      "2.277346611022949\n",
      "2.2769646644592285\n",
      "2.2765917778015137\n",
      "2.2762277126312256\n",
      "2.275872230529785\n",
      "2.275524616241455\n",
      "2.2751851081848145\n",
      "2.274853467941284\n",
      "2.274528741836548\n",
      "2.2742111682891846\n",
      "2.2739005088806152\n",
      "2.2735965251922607\n",
      "2.2732994556427\n",
      "2.2730085849761963\n",
      "2.27272367477417\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc.view(-1, 27*2) @ W # predict log-counts and merge both of the one-hot encoded character inputs\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexze.\n",
      "iogh.\n",
      "urailaziayh.\n",
      "elliimittain.\n",
      "lusan.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming W is initialized as a weight matrix with dimensions [54, 27]\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0  # Start with the start token `.`\n",
    "    \n",
    "    # Find the second token based on the first token being the start token `.`\n",
    "    p = (F.one_hot(torch.tensor([ix1]), num_classes=27).float() @ W[:27, :]).exp()\n",
    "    p = p / p.sum(1, keepdim=True)\n",
    "    ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix2])\n",
    "    \n",
    "    while True:\n",
    "        # Create a one-hot encoded vector for the pair of previous characters\n",
    "        xenc1 = F.one_hot(torch.tensor([ix1]), num_classes=27).float()\n",
    "        xenc2 = F.one_hot(torch.tensor([ix2]), num_classes=27).float()\n",
    "        \n",
    "        # Concatenate the two one-hot encoded vectors\n",
    "        xenc = torch.cat((xenc1, xenc2), dim=1)\n",
    "        \n",
    "        # Predict log-counts using the weight matrix\n",
    "        logits = xenc @ W  # W should have dimensions [54, 27]\n",
    "        counts = logits.exp()  # Convert log-counts to counts\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Sample the next character\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix3])\n",
    "        \n",
    "        if ix3 == 0:  # End token\n",
    "            break\n",
    "        \n",
    "        # Shift the indices for the next iteration\n",
    "        ix1, ix2 = ix2, ix3\n",
    "    \n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E02: Train-Dev-Test Set\n",
    "\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "words_train, words_test = train_test_split(words, test_size=0.2, random_state=1234)\n",
    "words_dev, words_test = train_test_split(words_test, test_size=0.5, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "x_train, y_train, x_dev, y_dev, x_test, y_test = [], [], [], [], [], []\n",
    "for wgroup in [words_train, words_dev, words_test]:\n",
    "    xs , ys = [], []\n",
    "    for w in wgroup:\n",
    "        # add start and end tokens\n",
    "        chs = [\".\"] + list(w) + [\".\"]\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "        \n",
    "            xs.append(ix1)\n",
    "            ys.append(ix2)\n",
    "\n",
    "    xs = torch.tensor(xs, dtype=torch.int64)\n",
    "    ys = torch.tensor(ys, dtype=torch.int64)\n",
    "\n",
    "    if wgroup == words_train:\n",
    "        x_train, y_train = xs, ys\n",
    "    elif wgroup == words_dev:\n",
    "        x_dev, y_dev = xs, ys\n",
    "    else:\n",
    "        x_test, y_test = xs, ys\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.767667293548584\n",
      "3.3780815601348877\n",
      "3.1604931354522705\n",
      "3.026573657989502\n",
      "2.9338600635528564\n",
      "2.8665993213653564\n",
      "2.8160202503204346\n",
      "2.776515483856201\n",
      "2.744630813598633\n",
      "2.718219041824341\n",
      "2.695909023284912\n",
      "2.67679500579834\n",
      "2.660250425338745\n",
      "2.6458210945129395\n",
      "2.6331593990325928\n",
      "2.621990919113159\n",
      "2.612090826034546\n",
      "2.603271722793579\n",
      "2.59537935256958\n",
      "2.588282823562622\n",
      "2.581873893737793\n",
      "2.5760607719421387\n",
      "2.570765733718872\n",
      "2.5659241676330566\n",
      "2.561481475830078\n",
      "2.5573911666870117\n",
      "2.5536131858825684\n",
      "2.550114154815674\n",
      "2.546865701675415\n",
      "2.543842315673828\n",
      "2.5410242080688477\n",
      "2.538391351699829\n",
      "2.535928249359131\n",
      "2.533620834350586\n",
      "2.5314555168151855\n",
      "2.529421329498291\n",
      "2.5275075435638428\n",
      "2.525705337524414\n",
      "2.524005889892578\n",
      "2.522402048110962\n",
      "2.5208864212036133\n",
      "2.5194523334503174\n",
      "2.518094301223755\n",
      "2.5168075561523438\n",
      "2.5155861377716064\n",
      "2.5144262313842773\n",
      "2.5133235454559326\n",
      "2.5122740268707275\n",
      "2.511274576187134\n",
      "2.510321617126465\n",
      "2.5094127655029297\n",
      "2.508544921875\n",
      "2.5077154636383057\n",
      "2.5069220066070557\n",
      "2.506162405014038\n",
      "2.5054352283477783\n",
      "2.5047380924224854\n",
      "2.5040690898895264\n",
      "2.503427267074585\n",
      "2.5028107166290283\n",
      "2.5022177696228027\n",
      "2.50164794921875\n",
      "2.5010993480682373\n",
      "2.5005714893341064\n",
      "2.5000622272491455\n",
      "2.4995718002319336\n",
      "2.4990992546081543\n",
      "2.498642683029175\n",
      "2.498202085494995\n",
      "2.497776508331299\n",
      "2.4973649978637695\n",
      "2.4969677925109863\n",
      "2.4965832233428955\n",
      "2.496211528778076\n",
      "2.4958512783050537\n",
      "2.4955027103424072\n",
      "2.4951653480529785\n",
      "2.494837999343872\n",
      "2.494520902633667\n",
      "2.494213581085205\n",
      "2.4939157962799072\n",
      "2.493626356124878\n",
      "2.4933454990386963\n",
      "2.493072986602783\n",
      "2.4928088188171387\n",
      "2.4925520420074463\n",
      "2.4923019409179688\n",
      "2.4920594692230225\n",
      "2.49182391166687\n",
      "2.4915943145751953\n",
      "2.4913713932037354\n",
      "2.491154670715332\n",
      "2.490943670272827\n",
      "2.4907383918762207\n",
      "2.4905383586883545\n",
      "2.4903438091278076\n",
      "2.490154266357422\n",
      "2.4899697303771973\n",
      "2.4897899627685547\n",
      "2.489614725112915\n",
      "2.4894442558288574\n",
      "2.4892776012420654\n",
      "2.4891154766082764\n",
      "2.488957405090332\n",
      "2.488802909851074\n",
      "2.4886527061462402\n",
      "2.4885056018829346\n",
      "2.4883625507354736\n",
      "2.488223075866699\n",
      "2.488086462020874\n",
      "2.4879531860351562\n",
      "2.487823247909546\n",
      "2.4876954555511475\n",
      "2.4875717163085938\n",
      "2.487450361251831\n",
      "2.4873321056365967\n",
      "2.4872164726257324\n",
      "2.4871034622192383\n",
      "2.486992597579956\n",
      "2.486884355545044\n",
      "2.486778736114502\n",
      "2.486675500869751\n",
      "2.486574172973633\n",
      "2.4864752292633057\n",
      "2.4863784313201904\n",
      "2.486283779144287\n",
      "2.4861910343170166\n",
      "2.486100196838379\n",
      "2.486011505126953\n",
      "2.485924482345581\n",
      "2.485839366912842\n",
      "2.485755681991577\n",
      "2.4856739044189453\n",
      "2.4855942726135254\n",
      "2.485515832901001\n",
      "2.485438823699951\n",
      "2.485363721847534\n",
      "2.4852898120880127\n",
      "2.485217332839966\n",
      "2.4851465225219727\n",
      "2.485076665878296\n",
      "2.485008716583252\n",
      "2.4849419593811035\n",
      "2.4848766326904297\n",
      "2.484812021255493\n",
      "2.4847490787506104\n",
      "2.484687089920044\n",
      "2.484626293182373\n",
      "2.4845666885375977\n",
      "2.4845080375671387\n",
      "2.484450578689575\n",
      "2.484394073486328\n",
      "2.4843387603759766\n",
      "2.4842844009399414\n",
      "2.4842309951782227\n",
      "2.484178304672241\n",
      "2.4841270446777344\n",
      "2.484076499938965\n",
      "2.4840266704559326\n",
      "2.483978271484375\n",
      "2.4839301109313965\n",
      "2.4838829040527344\n",
      "2.4838361740112305\n",
      "2.483790636062622\n",
      "2.48374605178833\n",
      "2.4837019443511963\n",
      "2.483658790588379\n",
      "2.4836158752441406\n",
      "2.483574151992798\n",
      "2.4835329055786133\n",
      "2.483492374420166\n",
      "2.483452558517456\n",
      "2.4834132194519043\n",
      "2.48337459564209\n",
      "2.4833366870880127\n",
      "2.4832992553710938\n",
      "2.483262538909912\n",
      "2.4832262992858887\n",
      "2.4831905364990234\n",
      "2.4831557273864746\n",
      "2.483120918273926\n",
      "2.4830870628356934\n",
      "2.483053684234619\n",
      "2.483020544052124\n",
      "2.4829883575439453\n",
      "2.4829559326171875\n",
      "2.482924699783325\n",
      "2.482893705368042\n",
      "2.482863187789917\n",
      "2.48283314704895\n",
      "2.4828033447265625\n",
      "2.482774257659912\n",
      "2.482745409011841\n",
      "2.4827170372009277\n",
      "2.482689142227173\n",
      "2.482661247253418\n",
      "2.4826343059539795\n",
      "2.48260760307312\n",
      "2.48258113861084\n",
      "2.4825551509857178\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(200):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(x_train, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(len(x_train)), y_train].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "def MLP_loss(x, y, W):\n",
    "    losses = []\n",
    "    for i in range(100):\n",
    "        xenc = F.one_hot(x, num_classes=27).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        loss = -probs[torch.arange(len(x)), y].log().mean() + 0.01*(W**2).mean()\n",
    "        losses.append(loss)\n",
    "    return sum(losses) / len(losses)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4826\n",
      "Dev Loss: 3.3618\n",
      "Test Loss: 3.3559\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Loss: {MLP_loss(x_train, y_train, W):.4f}\")\n",
    "print(f\"Dev Loss: {MLP_loss(x_dev, y_dev, W):.4f}\")\n",
    "print(f\"Test Loss: {MLP_loss(x_test, y_test, W):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_dev, y_dev, x_test, y_test = [], [], [], [], [], []\n",
    "for wgroup in [words_train, words_dev, words_test]:\n",
    "    xs , ys = [], []\n",
    "    for w in wgroup:\n",
    "        # add start and end tokens\n",
    "        chs = [\".\"] + list(w) + [\".\"]\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "        \n",
    "            xs.append([ix1, ix2])\n",
    "            ys.append(ix3)\n",
    "\n",
    "    xs = torch.tensor(xs, dtype=torch.int64)\n",
    "    ys = torch.tensor(ys, dtype=torch.int64)\n",
    "\n",
    "    if wgroup == words_train:\n",
    "        x_train, y_train = xs, ys\n",
    "    elif wgroup == words_dev:\n",
    "        x_dev, y_dev = xs, ys\n",
    "    else:\n",
    "        x_test, y_test = xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.186277866363525\n",
      "3.3571949005126953\n",
      "3.0424678325653076\n",
      "2.8718788623809814\n",
      "2.7675387859344482\n",
      "2.694955587387085\n",
      "2.6393167972564697\n",
      "2.595170736312866\n",
      "2.5591745376586914\n",
      "2.5293869972229004\n",
      "2.504401206970215\n",
      "2.483247756958008\n",
      "2.4651455879211426\n",
      "2.4495060443878174\n",
      "2.435850143432617\n",
      "2.423814058303833\n",
      "2.4131102561950684\n",
      "2.4035205841064453\n",
      "2.3948729038238525\n",
      "2.3870320320129395\n",
      "2.379887580871582\n",
      "2.373351573944092\n",
      "2.3673486709594727\n",
      "2.3618173599243164\n",
      "2.3567044734954834\n",
      "2.3519647121429443\n",
      "2.3475594520568848\n",
      "2.3434550762176514\n",
      "2.339622974395752\n",
      "2.3360371589660645\n",
      "2.3326756954193115\n",
      "2.3295187950134277\n",
      "2.3265492916107178\n",
      "2.3237509727478027\n",
      "2.321110963821411\n",
      "2.3186166286468506\n",
      "2.3162567615509033\n",
      "2.314021348953247\n",
      "2.311901330947876\n",
      "2.309887647628784\n",
      "2.3079748153686523\n",
      "2.3061537742614746\n",
      "2.3044190406799316\n",
      "2.302765369415283\n",
      "2.3011863231658936\n",
      "2.299678325653076\n",
      "2.2982358932495117\n",
      "2.2968556880950928\n",
      "2.2955329418182373\n",
      "2.294265031814575\n",
      "2.29304838180542\n",
      "2.291879892349243\n",
      "2.2907567024230957\n",
      "2.2896766662597656\n",
      "2.2886369228363037\n",
      "2.287635326385498\n",
      "2.286669969558716\n",
      "2.285738706588745\n",
      "2.2848398685455322\n",
      "2.2839713096618652\n",
      "2.283132314682007\n",
      "2.282320976257324\n",
      "2.2815356254577637\n",
      "2.280775547027588\n",
      "2.280038833618164\n",
      "2.279325485229492\n",
      "2.2786331176757812\n",
      "2.2779617309570312\n",
      "2.2773096561431885\n",
      "2.2766764163970947\n",
      "2.276061534881592\n",
      "2.275463819503784\n",
      "2.2748823165893555\n",
      "2.2743165493011475\n",
      "2.273766279220581\n",
      "2.2732303142547607\n",
      "2.2727081775665283\n",
      "2.2721993923187256\n",
      "2.2717034816741943\n",
      "2.2712202072143555\n",
      "2.2707483768463135\n",
      "2.2702882289886475\n",
      "2.26983904838562\n",
      "2.2694008350372314\n",
      "2.268972396850586\n",
      "2.268554449081421\n",
      "2.2681453227996826\n",
      "2.2677457332611084\n",
      "2.267354965209961\n",
      "2.2669732570648193\n",
      "2.266599178314209\n",
      "2.266233444213867\n",
      "2.2658751010894775\n",
      "2.2655248641967773\n",
      "2.26518177986145\n",
      "2.264845609664917\n",
      "2.264516592025757\n",
      "2.2641937732696533\n",
      "2.2638776302337646\n",
      "2.2635679244995117\n",
      "2.2632639408111572\n",
      "2.2629661560058594\n",
      "2.262673854827881\n",
      "2.26238751411438\n",
      "2.262106418609619\n",
      "2.2618300914764404\n",
      "2.2615597248077393\n",
      "2.261293649673462\n",
      "2.261033058166504\n",
      "2.2607765197753906\n",
      "2.2605249881744385\n",
      "2.26027774810791\n",
      "2.2600350379943848\n",
      "2.259796380996704\n",
      "2.2595620155334473\n",
      "2.259331464767456\n",
      "2.2591052055358887\n",
      "2.258882522583008\n",
      "2.2586636543273926\n",
      "2.258448600769043\n",
      "2.258236885070801\n",
      "2.258028984069824\n",
      "2.257824420928955\n",
      "2.257622718811035\n",
      "2.2574245929718018\n",
      "2.2572295665740967\n",
      "2.25703763961792\n",
      "2.2568488121032715\n",
      "2.2566630840301514\n",
      "2.2564799785614014\n",
      "2.2562999725341797\n",
      "2.256122350692749\n",
      "2.2559478282928467\n",
      "2.2557756900787354\n",
      "2.255606174468994\n",
      "2.255439281463623\n",
      "2.255274772644043\n",
      "2.255112886428833\n",
      "2.254953384399414\n",
      "2.254796028137207\n",
      "2.254640817642212\n",
      "2.254488229751587\n",
      "2.2543375492095947\n",
      "2.2541890144348145\n",
      "2.254042863845825\n",
      "2.2538983821868896\n",
      "2.253756046295166\n",
      "2.253615617752075\n",
      "2.253477096557617\n",
      "2.25334095954895\n",
      "2.253206491470337\n",
      "2.2530734539031982\n",
      "2.2529423236846924\n",
      "2.2528131008148193\n",
      "2.252685546875\n",
      "2.2525594234466553\n",
      "2.2524354457855225\n",
      "2.252312660217285\n",
      "2.2521917819976807\n",
      "2.252072334289551\n",
      "2.2519543170928955\n",
      "2.251837968826294\n",
      "2.251723051071167\n",
      "2.2516093254089355\n",
      "2.251497268676758\n",
      "2.251386880874634\n",
      "2.251277208328247\n",
      "2.251169204711914\n",
      "2.2510626316070557\n",
      "2.250957489013672\n",
      "2.2508535385131836\n",
      "2.2507505416870117\n",
      "2.2506489753723145\n",
      "2.2505486011505127\n",
      "2.2504494190216064\n",
      "2.2503514289855957\n",
      "2.2502546310424805\n",
      "2.2501590251922607\n",
      "2.2500641345977783\n",
      "2.2499704360961914\n",
      "2.249878406524658\n",
      "2.249786853790283\n",
      "2.2496964931488037\n",
      "2.2496070861816406\n",
      "2.249518632888794\n",
      "2.249431610107422\n",
      "2.249345541000366\n",
      "2.2492594718933105\n",
      "2.2491750717163086\n",
      "2.249091625213623\n",
      "2.249008893966675\n",
      "2.248927354812622\n",
      "2.2488467693328857\n",
      "2.2487664222717285\n",
      "2.248687267303467\n",
      "2.2486088275909424\n",
      "2.2485318183898926\n",
      "2.2484548091888428\n",
      "2.2483792304992676\n",
      "2.2483041286468506\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(200):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(x_train, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  xenc = xenc.view(-1, 27*2)\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(len(x_train)), y_train].log().mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "def MLP_loss(x, y, W):\n",
    "    losses = []\n",
    "    for i in range(100):\n",
    "        xenc = F.one_hot(x, num_classes=27).float()\n",
    "        xenc = xenc.view(-1, 27*2)\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        loss = -probs[torch.arange(len(x)), y].log().mean()\n",
    "        losses.append(loss)\n",
    "    return sum(losses) / len(losses)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2483\n",
      "Dev Loss: 3.3983\n",
      "Test Loss: 3.4116\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Loss: {MLP_loss(x_train, y_train, W):.4f}\")\n",
    "print(f\"Dev Loss: {MLP_loss(x_dev, y_dev, W):.4f}\")\n",
    "print(f\"Test Loss: {MLP_loss(x_test, y_test, W):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E03: Smoothing/Regularization for Trigram Model on Dev Set\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothness: 0 | Train Loss: 2.2378 | Dev Loss 3.4837\n",
      "smoothness: 0.01 | Train Loss: 2.2522 | Dev Loss 3.4109\n",
      "smoothness: 0.02 | Train Loss: 2.2630 | Dev Loss 3.3704\n",
      "smoothness: 0.05 | Train Loss: 2.2878 | Dev Loss 3.3030\n",
      "smoothness: 0.1 | Train Loss: 2.3184 | Dev Loss 3.2437\n",
      "smoothness: 0.25 | Train Loss: 2.3816 | Dev Loss 3.1590\n",
      "smoothness: 0.5 | Train Loss: 2.4517 | Dev Loss 3.0962\n",
      "smoothness: 1.0 | Train Loss: 2.5449 | Dev Loss 3.0422\n"
     ]
    }
   ],
   "source": [
    "smoothnesses = [0, 0.01, 0.02, 0.05, 0.1, 0.25, 0.5, 1.0]\n",
    "epochs = 1000\n",
    "for i, smoothness in enumerate(smoothnesses):\n",
    "  W = torch.randn((27*2,27), requires_grad = True)\n",
    "  for k in range(epochs):\n",
    "      # forward pass\n",
    "      xenc = F.one_hot(x_train, num_classes = 27).float()\n",
    "      xenc = xenc.view(-1, 27*2)\n",
    "      \n",
    "      # probs is softmax\n",
    "      logits = xenc @ W\n",
    "      counts = torch.exp(logits)\n",
    "      probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "      \n",
    "      # loss (normalized negative log likelihood)\n",
    "      loss = - probs[torch.arange(len(x_train)), y_train].log().mean()\n",
    "      # add regularization\n",
    "      loss += smoothness * W.pow(2).mean()\n",
    "\n",
    "      if k == epochs - 1:\n",
    "          print(f\"smoothness: {smoothness} | Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(x_dev, y_dev, W):.4f}\")\n",
    "\n",
    "      # backward pass\n",
    "      W.grad = None\n",
    "      loss.backward()\n",
    "\n",
    "      # update weights\n",
    "      with torch.no_grad():\n",
    "          W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test set on all settings of smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothness: 0 | Test Loss: 4.2632\n",
      "smoothness: 0.01 | Test Loss: 4.3308\n",
      "smoothness: 0.02 | Test Loss: 4.2541\n",
      "smoothness: 0.05 | Test Loss: 4.1176\n",
      "smoothness: 0.1 | Test Loss: 4.2537\n",
      "smoothness: 0.25 | Test Loss: 4.3165\n",
      "smoothness: 0.5 | Test Loss: 4.2331\n",
      "smoothness: 1.0 | Test Loss: 4.4352\n"
     ]
    }
   ],
   "source": [
    "smoothnesses = [0, 0.01, 0.02, 0.05, 0.1, 0.25, 0.5, 1.0]\n",
    "epochs = 1000\n",
    "for i, smoothness in enumerate(smoothnesses):\n",
    "  W = torch.randn((27*2,27), requires_grad = True)\n",
    "  for k in range(epochs):\n",
    "      # forward pass\n",
    "      xenc = F.one_hot(x_train, num_classes = 27).float()\n",
    "      xenc = xenc.view(-1, 27*2)\n",
    "      \n",
    "      # probs is softmax\n",
    "      logits = xenc @ W\n",
    "      counts = torch.exp(logits)\n",
    "      probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "      \n",
    "      # loss (normalized negative log likelihood)\n",
    "      loss = - probs[torch.arange(len(x_train)), y_train].log().mean()\n",
    "      # add regularization\n",
    "      loss += smoothness * W.pow(2).mean()\n",
    "\n",
    "      if k == epochs - 1:\n",
    "          print(f\"smoothness: {smoothness} | Test Loss: {MLP_loss(x_test, y_test, W):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E04: Replace F.one_hot With Indexing Into Rows Of W\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 13, 13,  1])\n",
      "tensor([[-0.8201, -0.1828,  0.5611,  1.5837, -0.2356,  0.5476,  0.7776, -0.6826,\n",
      "         -0.5316,  0.1229,  0.6429,  0.7000, -1.7489, -0.0120,  0.0371,  0.9434,\n",
      "         -0.1368, -0.2018, -0.2666,  1.6872,  1.6887,  1.3726, -0.4450, -0.3318,\n",
      "          1.7879, -1.2237,  0.1897],\n",
      "        [ 0.1886,  0.6403, -0.3606, -0.9040, -1.0911,  0.8286,  0.7166, -0.3046,\n",
      "         -0.3809, -0.9544,  0.2511,  0.3206, -0.4670, -0.9661, -1.3351, -1.1135,\n",
      "          0.5519,  0.6228,  0.3322, -1.6020, -2.4841, -0.5358, -0.1532, -0.5253,\n",
      "          1.0196,  0.0701, -0.5255],\n",
      "        [-1.2861, -0.8518,  0.7178,  0.3687, -0.8201, -0.7434,  0.3805,  0.2550,\n",
      "          0.8149,  1.0296, -0.1007,  0.4546, -1.1352, -1.9481, -0.3302,  0.0586,\n",
      "         -0.0091, -0.0868, -0.3421,  1.5494,  0.3669,  0.3609, -0.0476,  0.5742,\n",
      "          0.9386,  0.1258, -0.0483],\n",
      "        [-1.2861, -0.8518,  0.7178,  0.3687, -0.8201, -0.7434,  0.3805,  0.2550,\n",
      "          0.8149,  1.0296, -0.1007,  0.4546, -1.1352, -1.9481, -0.3302,  0.0586,\n",
      "         -0.0091, -0.0868, -0.3421,  1.5494,  0.3669,  0.3609, -0.0476,  0.5742,\n",
      "          0.9386,  0.1258, -0.0483],\n",
      "        [ 1.2157, -0.2629, -0.6341, -0.3618,  1.8328,  1.6726,  0.7331,  0.1157,\n",
      "         -1.8192,  0.3539,  0.5640,  0.5804, -0.0450, -1.1628,  0.3027,  0.2394,\n",
      "          0.0923,  1.6623,  0.4267,  0.2769,  2.3532, -0.0392,  0.0054, -1.3632,\n",
      "         -0.1526,  1.1599, -0.2825]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.8201, -0.1828,  0.5611,  1.5837, -0.2356,  0.5476,  0.7776, -0.6826,\n",
      "         -0.5316,  0.1229,  0.6429,  0.7000, -1.7489, -0.0120,  0.0371,  0.9434,\n",
      "         -0.1368, -0.2018, -0.2666,  1.6872,  1.6887,  1.3726, -0.4450, -0.3318,\n",
      "          1.7879, -1.2237,  0.1897],\n",
      "        [ 0.1886,  0.6403, -0.3606, -0.9040, -1.0911,  0.8286,  0.7166, -0.3046,\n",
      "         -0.3809, -0.9544,  0.2511,  0.3206, -0.4670, -0.9661, -1.3351, -1.1135,\n",
      "          0.5519,  0.6228,  0.3322, -1.6020, -2.4841, -0.5358, -0.1532, -0.5253,\n",
      "          1.0196,  0.0701, -0.5255],\n",
      "        [-1.2861, -0.8518,  0.7178,  0.3687, -0.8201, -0.7434,  0.3805,  0.2550,\n",
      "          0.8149,  1.0296, -0.1007,  0.4546, -1.1352, -1.9481, -0.3302,  0.0586,\n",
      "         -0.0091, -0.0868, -0.3421,  1.5494,  0.3669,  0.3609, -0.0476,  0.5742,\n",
      "          0.9386,  0.1258, -0.0483],\n",
      "        [-1.2861, -0.8518,  0.7178,  0.3687, -0.8201, -0.7434,  0.3805,  0.2550,\n",
      "          0.8149,  1.0296, -0.1007,  0.4546, -1.1352, -1.9481, -0.3302,  0.0586,\n",
      "         -0.0091, -0.0868, -0.3421,  1.5494,  0.3669,  0.3609, -0.0476,  0.5742,\n",
      "          0.9386,  0.1258, -0.0483],\n",
      "        [ 1.2157, -0.2629, -0.6341, -0.3618,  1.8328,  1.6726,  0.7331,  0.1157,\n",
      "         -1.8192,  0.3539,  0.5640,  0.5804, -0.0450, -1.1628,  0.3027,  0.2394,\n",
      "          0.0923,  1.6623,  0.4267,  0.2769,  2.3532, -0.0392,  0.0054, -1.3632,\n",
      "         -0.1526,  1.1599, -0.2825]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27,27), requires_grad = True)\n",
    "\n",
    "print(xs)\n",
    "xenc = F.one_hot(xs, num_classes = 27).float()\n",
    "logits = xenc @ W\n",
    "print(logits)\n",
    "\n",
    "logits = W[xs]\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  392226\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1, ix2))\n",
    "    ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1451, -1.9512,  0.8956,  ..., -0.1643,  0.2995,  2.1751],\n",
      "        [-0.9944,  0.7112,  1.4689,  ..., -0.1858,  0.5872,  0.2478],\n",
      "        [-3.0042,  1.0594, -0.6920,  ..., -0.9187, -0.8198,  0.8225],\n",
      "        ...,\n",
      "        [-2.0066, -0.4440, -1.7717,  ..., -0.9949, -1.5134,  2.6471],\n",
      "        [-0.0695, -0.9170,  3.1367,  ..., -2.0284, -1.4078,  0.3709],\n",
      "        [ 0.8476, -1.4958, -0.8498,  ...,  0.3662, -1.9539,  1.4925]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[ 0.1451, -1.9512,  0.8956,  ..., -0.1643,  0.2995,  2.1751],\n",
      "        [-0.9944,  0.7112,  1.4689,  ..., -0.1858,  0.5872,  0.2478],\n",
      "        [-3.0042,  1.0594, -0.6920,  ..., -0.9187, -0.8198,  0.8225],\n",
      "        ...,\n",
      "        [-2.0066, -0.4440, -1.7717,  ..., -0.9949, -1.5134,  2.6471],\n",
      "        [-0.0695, -0.9170,  3.1367,  ..., -2.0284, -1.4078,  0.3709],\n",
      "        [ 0.8476, -1.4958, -0.8498,  ...,  0.3662, -1.9539,  1.4925]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True)\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes = 27).float()\n",
    "xenc = xenc.view(-1, 27*2)\n",
    "logits = xenc @ W\n",
    "print(logits)\n",
    "\n",
    "logits = W[xs[:, 0]] + W[xs[:, 1]+27]\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E05: F.cross_entropy\n",
    "\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  392226\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1, ix2))\n",
    "    ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.186270713806152\n",
      "3.3573684692382812\n",
      "3.04215145111084\n",
      "2.871455192565918\n",
      "2.7671945095062256\n",
      "2.694681406021118\n",
      "2.6390926837921143\n",
      "2.5949814319610596\n",
      "2.55900239944458\n",
      "2.529222249984741\n",
      "2.5042335987091064\n",
      "2.483072519302368\n",
      "2.464961290359497\n",
      "2.4493143558502197\n",
      "2.435654401779175\n",
      "2.423619031906128\n",
      "2.412919521331787\n",
      "2.4033381938934326\n",
      "2.394700527191162\n",
      "2.386871337890625\n",
      "2.379739999771118\n",
      "2.3732173442840576\n",
      "2.3672289848327637\n",
      "2.3617119789123535\n",
      "2.3566133975982666\n",
      "2.3518881797790527\n",
      "2.34749698638916\n",
      "2.343407154083252\n",
      "2.339588165283203\n",
      "2.3360161781311035\n",
      "2.332667589187622\n",
      "2.3295228481292725\n",
      "2.3265655040740967\n",
      "2.3237788677215576\n",
      "2.3211493492126465\n",
      "2.3186655044555664\n",
      "2.3163156509399414\n",
      "2.314089059829712\n",
      "2.3119773864746094\n",
      "2.3099722862243652\n",
      "2.3080663681030273\n",
      "2.3062520027160645\n",
      "2.304523468017578\n",
      "2.302875518798828\n",
      "2.301302433013916\n",
      "2.2997987270355225\n",
      "2.298360586166382\n",
      "2.2969841957092285\n",
      "2.2956655025482178\n",
      "2.294400691986084\n",
      "2.293186902999878\n",
      "2.2920210361480713\n",
      "2.290900230407715\n",
      "2.2898218631744385\n",
      "2.2887840270996094\n",
      "2.2877843379974365\n",
      "2.28682017326355\n",
      "2.2858903408050537\n",
      "2.284992218017578\n",
      "2.284125328063965\n",
      "2.2832868099212646\n",
      "2.2824759483337402\n",
      "2.281691789627075\n",
      "2.2809317111968994\n",
      "2.280196189880371\n",
      "2.27948260307312\n",
      "2.2787907123565674\n",
      "2.2781195640563965\n",
      "2.277467966079712\n",
      "2.2768356800079346\n",
      "2.2762207984924316\n",
      "2.275623083114624\n",
      "2.2750420570373535\n",
      "2.2744765281677246\n",
      "2.2739264965057373\n",
      "2.273390769958496\n",
      "2.2728688716888428\n",
      "2.2723608016967773\n",
      "2.271864891052246\n",
      "2.2713818550109863\n",
      "2.2709109783172607\n",
      "2.270451068878174\n",
      "2.2700023651123047\n",
      "2.269564390182495\n",
      "2.269136428833008\n",
      "2.268718719482422\n",
      "2.268310308456421\n",
      "2.267911195755005\n",
      "2.2675209045410156\n",
      "2.267139196395874\n",
      "2.26676607131958\n",
      "2.2664005756378174\n",
      "2.266042947769165\n",
      "2.265693426132202\n",
      "2.265350818634033\n",
      "2.2650153636932373\n",
      "2.2646868228912354\n",
      "2.264364719390869\n",
      "2.2640492916107178\n",
      "2.263740062713623\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc.view(-1, 27*2) @ W # predict log-counts and merge both of the one-hot encoded character inputs\n",
    "    # counts = logits.exp() # counts, equivalent to N\n",
    "    # probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E06: Meta-Exercise\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n",
    "\n",
    "Reimplementing the MLP model using pytorch nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(27*2, 27)\n",
    "        # initialize weights with normal distribution with mean 0 and std 1\n",
    "        nn.init.normal_(self.fc1.weight, mean=0, std=1)\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        # Create one-hot encodings\n",
    "        xenc = F.one_hot(xs, num_classes=27).float()\n",
    "        xenc = xenc.view(-1, 27*2)\n",
    "        # Use the linear layer for the forward pass\n",
    "        logits = self.fc1(xenc)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.4355\n",
      "10: 3.5923\n",
      "20: 3.3295\n",
      "30: 3.1760\n",
      "40: 3.0714\n",
      "50: 2.9935\n",
      "60: 2.9320\n",
      "70: 2.8813\n",
      "80: 2.8385\n",
      "90: 2.8017\n",
      "100: 2.7695\n",
      "110: 2.7411\n",
      "120: 2.7157\n",
      "130: 2.6928\n",
      "140: 2.6722\n",
      "150: 2.6535\n",
      "160: 2.6363\n",
      "170: 2.6206\n",
      "180: 2.6062\n",
      "190: 2.5929\n",
      "Final loss: 2.58174729347229\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2)\n",
    "\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    logits = model(xs)\n",
    "    \n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "    # add regularization\n",
    "    loss += 0.2 * model.fc1.weight.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
