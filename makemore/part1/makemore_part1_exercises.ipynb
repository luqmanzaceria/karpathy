{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. With Counting\n",
    "2. With Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram With Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N = torch.zeros((27, 27 ,27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loss(input):\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for w in input:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "            prob = P[ix1, ix2, ix3]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "            #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "    print(f'{log_likelihood=}')\n",
    "    nll = -log_likelihood\n",
    "    print(f'{nll=}')\n",
    "    print(f\"{nll/n=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anuee.\n",
      "ova.\n",
      "amarbidushante.\n",
      "un.\n",
      "illayley.\n",
      "=========\n",
      "LOSS\n",
      "log_likelihood=tensor(-87.3249)\n",
      "nll=tensor(87.3249)\n",
      "nll/n=tensor(2.3601)\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "names = []\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0  # Start with the token '.'\n",
    "    \n",
    "    # First character after the start token\n",
    "    p = P[ix1, :].sum(0)  # Aggregate across all potential second characters\n",
    "    ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix2])\n",
    "\n",
    "    while True:\n",
    "        p = P[ix1, ix2]  # Get the probability distribution for the next character\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix3])\n",
    "        if ix3 == 0:  # End token\n",
    "            break\n",
    "        ix1, ix2 = ix2, ix3  # Move to the next character in the trigram\n",
    "    names.append(''.join(out))\n",
    "    print(''.join(out))\n",
    "\n",
    "print(\"=========\")\n",
    "print(\"LOSS\")\n",
    "count_loss(names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  392226\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1, ix2))\n",
    "    ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(42)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.220766544342041\n",
      "3.4673116207122803\n",
      "3.110934019088745\n",
      "2.9119250774383545\n",
      "2.7892463207244873\n",
      "2.703580856323242\n",
      "2.6405839920043945\n",
      "2.592454671859741\n",
      "2.5544941425323486\n",
      "2.523717164993286\n",
      "2.498292922973633\n",
      "2.4769906997680664\n",
      "2.4589340686798096\n",
      "2.443451166152954\n",
      "2.430039405822754\n",
      "2.4183120727539062\n",
      "2.407977342605591\n",
      "2.398803472518921\n",
      "2.390608072280884\n",
      "2.3832430839538574\n",
      "2.3765876293182373\n",
      "2.370542049407959\n",
      "2.365025043487549\n",
      "2.3599677085876465\n",
      "2.355313301086426\n",
      "2.3510138988494873\n",
      "2.3470299243927\n",
      "2.343327045440674\n",
      "2.3398759365081787\n",
      "2.3366518020629883\n",
      "2.3336329460144043\n",
      "2.3308002948760986\n",
      "2.3281373977661133\n",
      "2.325629711151123\n",
      "2.3232643604278564\n",
      "2.3210294246673584\n",
      "2.3189151287078857\n",
      "2.3169119358062744\n",
      "2.3150112628936768\n",
      "2.3132054805755615\n",
      "2.311487913131714\n",
      "2.3098528385162354\n",
      "2.308293581008911\n",
      "2.3068056106567383\n",
      "2.305384397506714\n",
      "2.3040246963500977\n",
      "2.3027234077453613\n",
      "2.30147647857666\n",
      "2.3002803325653076\n",
      "2.2991323471069336\n",
      "2.2980291843414307\n",
      "2.296969175338745\n",
      "2.2959482669830322\n",
      "2.294965982437134\n",
      "2.2940189838409424\n",
      "2.2931065559387207\n",
      "2.2922253608703613\n",
      "2.291374683380127\n",
      "2.290552854537964\n",
      "2.2897584438323975\n",
      "2.2889904975891113\n",
      "2.2882468700408936\n",
      "2.287527084350586\n",
      "2.286829948425293\n",
      "2.2861545085906982\n",
      "2.285499095916748\n",
      "2.2848641872406006\n",
      "2.284248113632202\n",
      "2.2836496829986572\n",
      "2.283068895339966\n",
      "2.2825045585632324\n",
      "2.281956672668457\n",
      "2.281423568725586\n",
      "2.2809057235717773\n",
      "2.2804019451141357\n",
      "2.279911518096924\n",
      "2.2794346809387207\n",
      "2.27897047996521\n",
      "2.2785181999206543\n",
      "2.278078317642212\n",
      "2.277649402618408\n",
      "2.2772319316864014\n",
      "2.276824474334717\n",
      "2.276427984237671\n",
      "2.27604079246521\n",
      "2.2756636142730713\n",
      "2.2752952575683594\n",
      "2.2749361991882324\n",
      "2.274585485458374\n",
      "2.274243116378784\n",
      "2.273909330368042\n",
      "2.27358341217041\n",
      "2.2732648849487305\n",
      "2.272953748703003\n",
      "2.2726497650146484\n",
      "2.272352933883667\n",
      "2.2720625400543213\n",
      "2.2717785835266113\n",
      "2.2715015411376953\n",
      "2.2712299823760986\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc.view(-1, 27*2) @ W # predict log-counts and merge both of the one-hot encoded character inputs\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anugeenvi.\n",
      "amarbian.\n",
      "dan.\n",
      "ubra.\n",
      "silayley.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming W is initialized as a weight matrix with dimensions [54, 27]\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0  # Start with the start token `.`\n",
    "    \n",
    "    # Find the second token based on the first token being the start token `.`\n",
    "    p = (F.one_hot(torch.tensor([ix1]), num_classes=27).float() @ W[:27, :]).exp()\n",
    "    p = p / p.sum(1, keepdim=True)\n",
    "    ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix2])\n",
    "    \n",
    "    while True:\n",
    "        # Create a one-hot encoded vector for the pair of previous characters\n",
    "        xenc1 = F.one_hot(torch.tensor([ix1]), num_classes=27).float()\n",
    "        xenc2 = F.one_hot(torch.tensor([ix2]), num_classes=27).float()\n",
    "        \n",
    "        # Concatenate the two one-hot encoded vectors\n",
    "        xenc = torch.cat((xenc1, xenc2), dim=1)\n",
    "        \n",
    "        # Predict log-counts using the weight matrix\n",
    "        logits = xenc @ W  # W should have dimensions [54, 27]\n",
    "        counts = logits.exp()  # Convert log-counts to counts\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        # Sample the next character\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix3])\n",
    "        \n",
    "        if ix3 == 0:  # End token\n",
    "            break\n",
    "        \n",
    "        # Shift the indices for the next iteration\n",
    "        ix1, ix2 = ix2, ix3\n",
    "    \n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
